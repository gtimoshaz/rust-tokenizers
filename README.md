# Tokenizers comparation

A small realization of different BPE's in terms of speed and do they produce the same result

## Algorithms

- work with strings and search for max length token each time
  - possible with Trie or with simple vector
- replace everything with numbers and search for their pairs

## Todo
- [ ] write a string-based tokenizer with vector as structure
- [ ] write bench
- [ ] write a numbers-based tokenizer with HashMap as structure
- [ ] everything else
